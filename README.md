<h2>勾配消失問題</h2>
活性化関数の導関数の最大値が1以下かつ、隠れ層の数が大きいときには起きる。<br>
又、各層の次元数が多い場合に活性化関数にシグモイドを使うと、勾配が消失しやすくなる。<br>
なぜなら、$ Wx+b $の値が大きくなるため、σ(x)->1から動かなくなるため。<br>

上記の事から、活性化関数の値やその導関数の値についての理解が必要。
<hr>
<h2>活性化関数の工夫</h2>

- 双曲線正接曲線（tanh）
<br>シグモイドよりも勾配消失が起きにくい

- ReLU
<br>$Wx$が0より大きければ1を返すため、勾配が消失することはない。<br>
一方で,$ Wx $が0以下の場合には0を返すので、そのニューロンは不活性となる。<br>
また、学習率が大きい場合には、学習過程でマイナスとなるため、注意が必要

- Leaky ReLU
<br>$Wx$が0以下の時も学習が進むので、ReLUよりも効果的な活性化関数と期待されているが、効果が起きるかはケースバイケース<br>
ReLUよりは計算コストがかかる

- Swish
<br>全範囲で微分可能であり、ReLUよりも高い精度を得ることができる。<br>
一方で、計算コストがReLUよりもあるため、得られる精度が著しく変わらない場合にはReLUを使うのが大半のケース<br>

以上の4つを用いてMnistに対して予測を行なったが、Sigmoidよりもいい結果だが、それぞれの精度に大きな違いはなかった。

<h2>ドロップアウトの導入</h2>
学習時にドロップアウトするニューロンはランダムのため、擬似的にアンサンプル学習を行う事になる。
<h3>注意事項</h3>

- ドロップアウトのするには学習時のみで、テスト時にはしない。<br>
Pytorchの場合には、テスト時にドロップアウトをしないようにtrain()とeval()を使い分ける
- ドロップアウトするニューロンの出力は$1/1-p$にスケーリングする